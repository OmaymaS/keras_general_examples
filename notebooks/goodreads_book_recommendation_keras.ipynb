{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "goodreads-book-recommendation-keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "BhBXkLhL39SV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook provides the code presented in the blog post [Collaborative Filtering Using Embeddings](https://www.onceupondata.com/2019/02/01/nn-collaborative-filtering/).  We will use the [goodbooks-10k](https://github.com/zygmuntz/goodbooks-10k) dataset, which contains *\" around six million ratings for ten thousand books\"*.  The post will explain  the high-level concepts then it will show preliminary **Keras** models to demonstrate two methods in which we can use embeddings to represent our data. We will refer to the two methods as:\n",
        "\n",
        "-  Matrix factorization method\n",
        "-  Tabular data method \n",
        "\n",
        "**Note that the examples are for demo purposes and they are not intended to provide the best model. Also I am not aware of a benchmark for this dataset, so the comparison with other models is out of the scope*"
      ]
    },
    {
      "metadata": {
        "id": "2UPxvFQs7klD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization, Latent Factors and Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "jb5dK4vy7v7L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a platform like goodreads, we see book recommendations based on the history of the user, similarities with other users or other signals collected about the user behavior. In practice, different signals and algorithms are usually merged to get better results. But let's focus on one approach; **item-based collaborative filtering**. Using this approach, we learn about the similarities between the books using the ratings we already have. Starting with tabular data, the first step is to convert our table to a matrix with one row per user as shown in the following figure. "
      ]
    },
    {
      "metadata": {
        "id": "8NtAkVPD_2Ut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/OmaymaS/onceupondata/blob/master/static/post/2019-02-01_goodreads-book-recommendation/table_to_matrix.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "WpnESbhO_ykl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**But what is the problem with that?**\n",
        "\n",
        "Usually we'll have thousands of users and items, so this matrix will be very sparse with dimensions **(mXn)** .\n",
        "\n",
        "**What is the common solution?**\n",
        "\n",
        "To reduce the complexity of the problem, it is common to deal with lower-dimension matrices using matrix factorization. The idea is to represet each user/item by a compact vector represting the latent factors. \n",
        "\n",
        "**And How can embeddings help?**\n",
        "\n",
        "Instead of the traditional ways, we can use embedding layers and learn about the latent factors while training our neural network using the given ratings. As a result we will have:\n",
        "\n",
        "- Users embedding **(mXk)**\n",
        "- Items embedding **(nXk)** \n",
        "\n",
        "Ideally, with a good model, users/items close to each other in the space should have similar characteristics."
      ]
    },
    {
      "metadata": {
        "id": "N-fpixGdLmxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ![](https://github.com/OmaymaS/onceupondata/blob/master/static/post/2019-02-01_goodreads-book-recommendation/embedding_matrix.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "83zxvoS3FNWG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we can use these matrices in two ways:\n",
        "\n",
        "1-  **Matrix factorization method**  where we take these matrices, calculate the element-wise product of **(mXk),(kXm)** then add other fully connected layers.\n"
      ]
    },
    {
      "metadata": {
        "id": "QkeNPpUZBiLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ![](https://github.com/OmaymaS/onceupondata/blob/master/static/post/2019-02-01_goodreads-book-recommendation/matrix_to_embedding2.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "wTPuh5cnNMl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2 - **Tabular data method** where we use the embedding matrices as lookup tables to get the vectors corresponding to each user/item. These vectors will be consideres as features and we can add other fully connected layers, or even try something else other than neural networks."
      ]
    },
    {
      "metadata": {
        "id": "hOJwHgppLtnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ![](https://github.com/OmaymaS/onceupondata/blob/master/static/post/2019-02-01_goodreads-book-recommendation/table_to_tabular.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "W6SC0mEJOQXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Yn5P1PlHOu0K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now as we got the idea behind using embeddings, let's start with loading the goodreads `ratings` dataset. Notice that it is similar to the illustrated example above, where we have **user_id, book_id, rating**. "
      ]
    },
    {
      "metadata": {
        "id": "GwnOMggPOUO1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## import libraries\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "biniG-jGQsyb",
        "colab_type": "code",
        "outputId": "8df8ff94-eaf1-4df8-9de7-07d7bec5107f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "## read data\n",
        "ratings = pd.read_csv(\"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/ratings.csv\")\n",
        "books = pd.read_csv(\"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv\")\n",
        "\n",
        "ratings.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>258</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4081</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>260</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>9296</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2318</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  book_id  rating\n",
              "0        1      258       5\n",
              "1        2     4081       4\n",
              "2        2      260       5\n",
              "3        2     9296       5\n",
              "4        2     2318       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "5q3wioGsQKK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Unique users/books"
      ]
    },
    {
      "metadata": {
        "id": "fmHfF_ojPy8U",
        "colab_type": "code",
        "outputId": "30ffd90c-0701-4a28-c27e-e4c5a5eeb6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## unisue users, books\n",
        "n_users, n_books = len(ratings.user_id.unique()), len(ratings.book_id.unique())\n",
        "\n",
        "\n",
        "f'The dataset includes {len(ratings)} ratings by {n_users} unique users on {n_books} unique books.'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The dataset includes 5976479 ratings by 53424 unique users on 10000 unique books.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "7rGVLQHlQVPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Rating distribution"
      ]
    },
    {
      "metadata": {
        "id": "uYtDRl-5RIOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we look at the distribution of ratings, We can notice that:\n",
        "\n",
        "- the values are discrete (1 to 5), but we will deal with them as continuous as we are ok with predicting intermediate values.\n",
        "\n",
        "- the data is unbalanced, and most of the ratings are from 3 to 5. (*Ideally we should consier the impact of such imbalance on the model*)"
      ]
    },
    {
      "metadata": {
        "id": "fqH36jn4fN_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1472ef17-014e-4f64-ead4-075fc4a9dc41"
      },
      "cell_type": "code",
      "source": [
        "## plot a histogram of ratings\n",
        "plt.hist(ratings['rating'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 124195.,       0.,  359257.,       0.,       0., 1370916.,\n",
              "              0., 2139018.,       0., 1983093.]),\n",
              " array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD9CAYAAABN7FvjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QXnnk7hKUfYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train/Test Split"
      ]
    },
    {
      "metadata": {
        "id": "YTv2U2XkU23Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can look further into the dataset and select books with number of reviews above a certain threshold, or exclude certain records. But for simplicity, we will use the data as is and keep 10% for testing."
      ]
    },
    {
      "metadata": {
        "id": "kVLNOXNdUcyt",
        "colab_type": "code",
        "outputId": "2ac9bdbf-180f-42ba-871c-da8cafc7d346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## split the data to train and test dataframes\n",
        "train, test = train_test_split(ratings, test_size = 0.1)\n",
        "\n",
        "f\"The training and testing data include {len(train), len(test)} records.\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The training and testing data include (5378831, 597648) records.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "529aPvcGN4kE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras Models"
      ]
    },
    {
      "metadata": {
        "id": "Bts0z3u1fwUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we have our data ready, we can create Keras models to implement the concepts discussed above. We will use the functional API as we need to bulid multi-input models."
      ]
    },
    {
      "metadata": {
        "id": "N-EzqxQXXD6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## import keras models, layers and optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Flatten, Dense, Dropout, concatenate, multiply, Input\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "## for model block diagram visualization\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YJ5GLCaN8wb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1- Matrix factorization method"
      ]
    },
    {
      "metadata": {
        "id": "gqxrVfzsnWz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The core of this methods is having:\n",
        "\n",
        "- **users and books embedding layers** with the same number of factors.\n",
        "- **user and books bias layers** which can be considered as representation of the unique inherent characteristic of each user/book.\n",
        "\n",
        "We will take the element-wise product of the two embeddings and add the bias terms. The rest is all about experimenting with layers and tuning hyper parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "9uthmEtzgQWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create model"
      ]
    },
    {
      "metadata": {
        "id": "aX9JUKj3bPuH",
        "colab_type": "code",
        "outputId": "0b816b3d-e0ba-4805-cfd2-e9b2c82a2ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "cell_type": "code",
      "source": [
        "## define embedding size (similar for both users and books)\n",
        "dim_embedddings = 32\n",
        "bias = 1\n",
        "\n",
        "# books\n",
        "book_input = Input(shape=[1],name='Book')\n",
        "book_embedding = Embedding(n_books+1, dim_embedddings, name=\"Book-Embedding\")(book_input)\n",
        "book_bias = Embedding(n_books + 1, bias, name=\"Book-Bias\")(book_input)\n",
        "\n",
        "# users\n",
        "user_input = Input(shape=[1],name='User')\n",
        "user_embedding = Embedding(n_users+1, dim_embedddings, name=\"User-Embedding\")(user_input)\n",
        "user_bias = Embedding(n_users + 1, bias, name=\"User-Bias\")(user_input)\n",
        "\n",
        "\n",
        "matrix_product = multiply([book_embedding, user_embedding])\n",
        "matrix_product = Dropout(0.2)(matrix_product)\n",
        "\n",
        "input_terms = concatenate([matrix_product, user_bias, book_bias])\n",
        "input_terms = Flatten()(input_terms)\n",
        "\n",
        "## add dense layers\n",
        "dense_0 = Dense(128, activation=\"relu\", name = \"Dense0\")(input_terms)\n",
        "dense_1 = Dropout(0.2)(dense_0)\n",
        "dense_1 = Dense(64, activation=\"relu\", name = \"Dense1\")(dense_0)\n",
        "dense_1 = Dropout(0.2)(dense_1)\n",
        "dense_2 = Dense(32, activation=\"relu\", name = \"Dense2\")(dense_1)\n",
        "dense_2 = Dropout(0.2)(dense_2)\n",
        "result = Dense(1, activation='relu', name='Activation')(dense_2)\n",
        "\n",
        "## define model with 2 inputs and 1 output\n",
        "model_mf = Model(inputs=[book_input, user_input], outputs = result)\n",
        "\n",
        "## show model summary\n",
        "model_mf.summary()\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Book (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "User (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Book-Embedding (Embedding)      (None, 1, 32)        320032      Book[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "User-Embedding (Embedding)      (None, 1, 32)        1709600     User[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 1, 32)        0           Book-Embedding[0][0]             \n",
            "                                                                 User-Embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 1, 32)        0           multiply_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "User-Bias (Embedding)           (None, 1, 1)         53425       User[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "Book-Bias (Embedding)           (None, 1, 1)         10001       Book[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 1, 34)        0           dropout_19[0][0]                 \n",
            "                                                                 User-Bias[0][0]                  \n",
            "                                                                 Book-Bias[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 34)           0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Dense0 (Dense)                  (None, 128)          4480        flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Dense1 (Dense)                  (None, 64)           8256        Dense0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 64)           0           Dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Dense2 (Dense)                  (None, 32)           2080        dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 32)           0           Dense2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Activation (Dense)              (None, 1)            33          dropout_22[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,107,907\n",
            "Trainable params: 2,107,907\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-dPaLVuFgWEN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Show model diagram"
      ]
    },
    {
      "metadata": {
        "id": "0z0ChNYEfOih",
        "colab_type": "code",
        "outputId": "74a91716-1647-4491-98bd-f74fdb8cee7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1152
        }
      },
      "cell_type": "code",
      "source": [
        "SVG(model_to_dot(model_mf).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"848pt\" viewBox=\"0.00 0.00 534.00 848.00\" width=\"534pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 844)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-844 530,-844 530,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139737309692480 -->\n<g class=\"node\" id=\"node1\">\n<title>139737309692480</title>\n<polygon fill=\"none\" points=\"12.5,-803.5 12.5,-839.5 132.5,-839.5 132.5,-803.5 12.5,-803.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72.5\" y=\"-817.8\">Book: InputLayer</text>\n</g>\n<!-- 139737309692256 -->\n<g class=\"node\" id=\"node3\">\n<title>139737309692256</title>\n<polygon fill=\"none\" points=\"37.5,-730.5 37.5,-766.5 233.5,-766.5 233.5,-730.5 37.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135.5\" y=\"-744.8\">Book-Embedding: Embedding</text>\n</g>\n<!-- 139737309692480&#45;&gt;139737309692256 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139737309692480-&gt;139737309692256</title>\n<path d=\"M88.073,-803.4551C95.6452,-794.6809 104.8838,-783.9759 113.1885,-774.353\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"116.0039,-776.4477 119.8878,-766.5904 110.7045,-771.8742 116.0039,-776.4477\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692704 -->\n<g class=\"node\" id=\"node8\">\n<title>139737309692704</title>\n<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 153,-620.5 153,-584.5 0,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"76.5\" y=\"-598.8\">Book-Bias: Embedding</text>\n</g>\n<!-- 139737309692480&#45;&gt;139737309692704 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139737309692480-&gt;139737309692704</title>\n<path d=\"M52.5559,-803.3483C43.3327,-793.5943 33.361,-780.7592 28.5,-767 11.6514,-719.3091 38.3528,-662.4091 58.1903,-629.5665\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"61.2315,-631.3043 63.5615,-620.9688 55.2948,-627.5954 61.2315,-631.3043\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309693936 -->\n<g class=\"node\" id=\"node2\">\n<title>139737309693936</title>\n<polygon fill=\"none\" points=\"350.5,-803.5 350.5,-839.5 464.5,-839.5 464.5,-803.5 350.5,-803.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"407.5\" y=\"-817.8\">User: InputLayer</text>\n</g>\n<!-- 139737309692424 -->\n<g class=\"node\" id=\"node4\">\n<title>139737309692424</title>\n<polygon fill=\"none\" points=\"251.5,-730.5 251.5,-766.5 441.5,-766.5 441.5,-730.5 251.5,-730.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346.5\" y=\"-744.8\">User-Embedding: Embedding</text>\n</g>\n<!-- 139737309693936&#45;&gt;139737309692424 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139737309693936-&gt;139737309692424</title>\n<path d=\"M392.4213,-803.4551C385.0895,-794.6809 376.1443,-783.9759 368.1032,-774.353\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"370.7146,-772.0197 361.6166,-766.5904 365.3431,-776.5082 370.7146,-772.0197\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692928 -->\n<g class=\"node\" id=\"node7\">\n<title>139737309692928</title>\n<polygon fill=\"none\" points=\"379,-657.5 379,-693.5 526,-693.5 526,-657.5 379,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.5\" y=\"-671.8\">User-Bias: Embedding</text>\n</g>\n<!-- 139737309693936&#45;&gt;139737309692928 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139737309693936-&gt;139737309692928</title>\n<path d=\"M426.8726,-803.2227C435.8559,-793.4366 445.6098,-780.6108 450.5,-767 457.7756,-746.7499 457.9134,-722.3194 456.4291,-703.798\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"459.8919,-703.2518 455.4095,-693.6519 452.927,-703.9518 459.8919,-703.2518\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692760 -->\n<g class=\"node\" id=\"node5\">\n<title>139737309692760</title>\n<polygon fill=\"none\" points=\"205.5,-657.5 205.5,-693.5 343.5,-693.5 343.5,-657.5 205.5,-657.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-671.8\">multiply_5: Multiply</text>\n</g>\n<!-- 139737309692256&#45;&gt;139737309692760 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139737309692256-&gt;139737309692760</title>\n<path d=\"M169.8596,-730.4551C188.3207,-720.7596 211.271,-708.7066 230.9701,-698.361\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"232.828,-701.3387 240.054,-693.5904 229.5733,-695.1413 232.828,-701.3387\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692424&#45;&gt;139737309692760 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139737309692424-&gt;139737309692760</title>\n<path d=\"M328.7022,-730.4551C319.8752,-721.5054 309.0669,-710.547 299.4307,-700.7769\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"301.8566,-698.2523 292.3425,-693.5904 296.8728,-703.1678 301.8566,-698.2523\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309694888 -->\n<g class=\"node\" id=\"node6\">\n<title>139737309694888</title>\n<polygon fill=\"none\" points=\"204,-584.5 204,-620.5 345,-620.5 345,-584.5 204,-584.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-598.8\">dropout_19: Dropout</text>\n</g>\n<!-- 139737309692760&#45;&gt;139737309694888 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139737309692760-&gt;139737309694888</title>\n<path d=\"M274.5,-657.4551C274.5,-649.3828 274.5,-639.6764 274.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-630.5903 274.5,-620.5904 271.0001,-630.5904 278.0001,-630.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692984 -->\n<g class=\"node\" id=\"node9\">\n<title>139737309692984</title>\n<polygon fill=\"none\" points=\"187,-511.5 187,-547.5 362,-547.5 362,-511.5 187,-511.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-525.8\">concatenate_7: Concatenate</text>\n</g>\n<!-- 139737309694888&#45;&gt;139737309692984 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139737309694888-&gt;139737309692984</title>\n<path d=\"M274.5,-584.4551C274.5,-576.3828 274.5,-566.6764 274.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-557.5903 274.5,-547.5904 271.0001,-557.5904 278.0001,-557.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692928&#45;&gt;139737309692984 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139737309692928-&gt;139737309692984</title>\n<path d=\"M434.8284,-657.429C415.7593,-638.3123 384.1314,-607.6968 354.5,-584 341.1497,-573.3235 325.7783,-562.527 312.0997,-553.3915\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"313.6678,-550.2328 303.3936,-547.6459 309.8121,-556.0753 313.6678,-550.2328\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692704&#45;&gt;139737309692984 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139737309692704-&gt;139737309692984</title>\n<path d=\"M125.4438,-584.4551C152.812,-574.3648 187.106,-561.721 215.8977,-551.1059\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"217.2611,-554.3336 225.433,-547.5904 214.8396,-547.7658 217.2611,-554.3336\" stroke=\"#000000\"/>\n</g>\n<!-- 139737309692816 -->\n<g class=\"node\" id=\"node10\">\n<title>139737309692816</title>\n<polygon fill=\"none\" points=\"218,-438.5 218,-474.5 331,-474.5 331,-438.5 218,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-452.8\">flatten_5: Flatten</text>\n</g>\n<!-- 139737309692984&#45;&gt;139737309692816 -->\n<g class=\"edge\" id=\"edge11\">\n<title>139737309692984-&gt;139737309692816</title>\n<path d=\"M274.5,-511.4551C274.5,-503.3828 274.5,-493.6764 274.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-484.5903 274.5,-474.5904 271.0001,-484.5904 278.0001,-484.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736941187984 -->\n<g class=\"node\" id=\"node11\">\n<title>139736941187984</title>\n<polygon fill=\"none\" points=\"223.5,-365.5 223.5,-401.5 325.5,-401.5 325.5,-365.5 223.5,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-379.8\">Dense0: Dense</text>\n</g>\n<!-- 139737309692816&#45;&gt;139736941187984 -->\n<g class=\"edge\" id=\"edge12\">\n<title>139737309692816-&gt;139736941187984</title>\n<path d=\"M274.5,-438.4551C274.5,-430.3828 274.5,-420.6764 274.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-411.5903 274.5,-401.5904 271.0001,-411.5904 278.0001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736935270104 -->\n<g class=\"node\" id=\"node12\">\n<title>139736935270104</title>\n<polygon fill=\"none\" points=\"223.5,-292.5 223.5,-328.5 325.5,-328.5 325.5,-292.5 223.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-306.8\">Dense1: Dense</text>\n</g>\n<!-- 139736941187984&#45;&gt;139736935270104 -->\n<g class=\"edge\" id=\"edge13\">\n<title>139736941187984-&gt;139736935270104</title>\n<path d=\"M274.5,-365.4551C274.5,-357.3828 274.5,-347.6764 274.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-338.5903 274.5,-328.5904 271.0001,-338.5904 278.0001,-338.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736935272288 -->\n<g class=\"node\" id=\"node13\">\n<title>139736935272288</title>\n<polygon fill=\"none\" points=\"204,-219.5 204,-255.5 345,-255.5 345,-219.5 204,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-233.8\">dropout_21: Dropout</text>\n</g>\n<!-- 139736935270104&#45;&gt;139736935272288 -->\n<g class=\"edge\" id=\"edge14\">\n<title>139736935270104-&gt;139736935272288</title>\n<path d=\"M274.5,-292.4551C274.5,-284.3828 274.5,-274.6764 274.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-265.5903 274.5,-255.5904 271.0001,-265.5904 278.0001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736935290192 -->\n<g class=\"node\" id=\"node14\">\n<title>139736935290192</title>\n<polygon fill=\"none\" points=\"223.5,-146.5 223.5,-182.5 325.5,-182.5 325.5,-146.5 223.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-160.8\">Dense2: Dense</text>\n</g>\n<!-- 139736935272288&#45;&gt;139736935290192 -->\n<g class=\"edge\" id=\"edge15\">\n<title>139736935272288-&gt;139736935290192</title>\n<path d=\"M274.5,-219.4551C274.5,-211.3828 274.5,-201.6764 274.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-192.5903 274.5,-182.5904 271.0001,-192.5904 278.0001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736935290416 -->\n<g class=\"node\" id=\"node15\">\n<title>139736935290416</title>\n<polygon fill=\"none\" points=\"204,-73.5 204,-109.5 345,-109.5 345,-73.5 204,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-87.8\">dropout_22: Dropout</text>\n</g>\n<!-- 139736935290192&#45;&gt;139736935290416 -->\n<g class=\"edge\" id=\"edge16\">\n<title>139736935290192-&gt;139736935290416</title>\n<path d=\"M274.5,-146.4551C274.5,-138.3828 274.5,-128.6764 274.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-119.5903 274.5,-109.5904 271.0001,-119.5904 278.0001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736936110176 -->\n<g class=\"node\" id=\"node16\">\n<title>139736936110176</title>\n<polygon fill=\"none\" points=\"215,-.5 215,-36.5 334,-36.5 334,-.5 215,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-14.8\">Activation: Dense</text>\n</g>\n<!-- 139736935290416&#45;&gt;139736936110176 -->\n<g class=\"edge\" id=\"edge17\">\n<title>139736935290416-&gt;139736936110176</title>\n<path d=\"M274.5,-73.4551C274.5,-65.3828 274.5,-55.6764 274.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"278.0001,-46.5903 274.5,-36.5904 271.0001,-46.5904 278.0001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "X14sjxgVgcxk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train model"
      ]
    },
    {
      "metadata": {
        "id": "mpVoWdI_wui4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will initially train the model for 4 epochs and monitor  the loss on the training and validation data."
      ]
    },
    {
      "metadata": {
        "id": "-DAXzuM8hLWQ",
        "colab_type": "code",
        "outputId": "a02ac533-3b4d-4bb4-defb-c4bae044ff58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "## specify learning rate (or use the default)\n",
        "opt_adam = Adam(lr = 0.002)\n",
        "\n",
        "## compile model\n",
        "model_mf.compile(optimizer = opt_adam, loss = ['mse'], metrics = ['mean_absolute_error'])\n",
        "\n",
        "## fit model\n",
        "history_mf = model_mf.fit([train['user_id'], train['book_id']],\n",
        "                          train['rating'],\n",
        "                          batch_size = 256,\n",
        "                          validation_split = 0.005,\n",
        "                          epochs = 16,\n",
        "                          verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5351936 samples, validate on 26895 samples\n",
            "Epoch 1/16\n",
            "5351936/5351936 [==============================] - 206s 39us/step - loss: 0.9524 - mean_absolute_error: 0.7776 - val_loss: 0.8873 - val_mean_absolute_error: 0.7417\n",
            "Epoch 2/16\n",
            "5351936/5351936 [==============================] - 205s 38us/step - loss: 0.8866 - mean_absolute_error: 0.7499 - val_loss: 0.8807 - val_mean_absolute_error: 0.7451\n",
            "Epoch 3/16\n",
            "5351936/5351936 [==============================] - 205s 38us/step - loss: 0.8768 - mean_absolute_error: 0.7446 - val_loss: 0.8793 - val_mean_absolute_error: 0.7427\n",
            "Epoch 4/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8732 - mean_absolute_error: 0.7404 - val_loss: 0.8784 - val_mean_absolute_error: 0.7434\n",
            "Epoch 5/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8627 - mean_absolute_error: 0.7349 - val_loss: 0.8784 - val_mean_absolute_error: 0.7419\n",
            "Epoch 6/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8590 - mean_absolute_error: 0.7316 - val_loss: 0.8786 - val_mean_absolute_error: 0.7452\n",
            "Epoch 7/16\n",
            "5351936/5351936 [==============================] - 202s 38us/step - loss: 0.8526 - mean_absolute_error: 0.7291 - val_loss: 0.8816 - val_mean_absolute_error: 0.7421\n",
            "Epoch 8/16\n",
            "5351936/5351936 [==============================] - 202s 38us/step - loss: 0.8489 - mean_absolute_error: 0.7271 - val_loss: 0.8800 - val_mean_absolute_error: 0.7449\n",
            "Epoch 9/16\n",
            "5351936/5351936 [==============================] - 206s 39us/step - loss: 0.8462 - mean_absolute_error: 0.7255 - val_loss: 0.8791 - val_mean_absolute_error: 0.7443\n",
            "Epoch 10/16\n",
            "5351936/5351936 [==============================] - 206s 38us/step - loss: 0.8443 - mean_absolute_error: 0.7243 - val_loss: 0.8804 - val_mean_absolute_error: 0.7455\n",
            "Epoch 11/16\n",
            "5351936/5351936 [==============================] - 209s 39us/step - loss: 0.8414 - mean_absolute_error: 0.7231 - val_loss: 0.8812 - val_mean_absolute_error: 0.7446\n",
            "Epoch 12/16\n",
            "5351936/5351936 [==============================] - 209s 39us/step - loss: 0.8450 - mean_absolute_error: 0.7222 - val_loss: 0.8819 - val_mean_absolute_error: 0.7479\n",
            "Epoch 13/16\n",
            "5351936/5351936 [==============================] - 207s 39us/step - loss: 0.8397 - mean_absolute_error: 0.7224 - val_loss: 0.8821 - val_mean_absolute_error: 0.7465\n",
            "Epoch 14/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8384 - mean_absolute_error: 0.7218 - val_loss: 0.8822 - val_mean_absolute_error: 0.7435\n",
            "Epoch 15/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8372 - mean_absolute_error: 0.7211 - val_loss: 0.8826 - val_mean_absolute_error: 0.7474\n",
            "Epoch 16/16\n",
            "5351936/5351936 [==============================] - 203s 38us/step - loss: 0.8359 - mean_absolute_error: 0.7204 - val_loss: 0.8837 - val_mean_absolute_error: 0.7462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8c6b5z98bA9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Interpretation of embedding/bias values"
      ]
    },
    {
      "metadata": {
        "id": "kTeemDjwAAj1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ideally, if we have trained the model enough to learn about the latent factors of the users/books, the values of the bias or embedding matrices should have a meaning about the underlying characteristcs of what they represent. Although this model is preliminary, we will see how we could interprent these values with a good model."
      ]
    },
    {
      "metadata": {
        "id": "iaZj__rJAriv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we will create a dictionary with the book id and title."
      ]
    },
    {
      "metadata": {
        "id": "QMhr9QRN8VLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## create a dictionary out of bookid, book original title\n",
        "books_dict = books.set_index('book_id')['original_title'].to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fu8LXRv5A0Cg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we will extract the book embedding weights, and we can see it has the expected dimensions."
      ]
    },
    {
      "metadata": {
        "id": "G7W2XXuQYBSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "B3nJ10ur-tcN",
        "colab_type": "code",
        "outputId": "cddfe37b-2a50-49ad-99f9-d3deae4baaca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "book_embedding_weights = model_mf.layers[2].get_weights()[0]\n",
        "book_embedding_weights.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10001, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "egqBTW7RA-pw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To compress these these vectors into less dimensions, we can use **Principal Component Analysis(PCA)**, to reduce the `dim_embedddings` to three components."
      ]
    },
    {
      "metadata": {
        "id": "1AL1sb0J-26F",
        "colab_type": "code",
        "outputId": "15de35d5-0ac4-4da5-e82b-48ae73980cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 3) ## use 3 components\n",
        "book_embedding_weights_t = np.transpose(book_embedding_weights) ## pass the transpose of the embedding matrix\n",
        "book_pca = pca.fit(book_embedding_weights_t) ## fit\n",
        "\n",
        "## display the resulting matrix dimensions\n",
        "book_pca.components_.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 10001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "rYqxQGP9CfHR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can look at the percentage of variance explained by each of the selected components."
      ]
    },
    {
      "metadata": {
        "id": "YNpJutCaBLLU",
        "colab_type": "code",
        "outputId": "915b887c-0aab-4b11-a9fc-f2dc0555e06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## display the variance explained by the 3 components\n",
        "book_pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05997574, 0.04405401, 0.04194602], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "EL-wBtpACy2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the variance explained is very low, we might not be able to see a good interpretation. However, for demo purposes, we will just extract the first component/factor that explains the highest percentage of the variance. The array we get can be mapped to the books list."
      ]
    },
    {
      "metadata": {
        "id": "FzKqkbO4_Ps1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "## extract first PCA\n",
        "pca0 = book_pca.components_[0]\n",
        "\n",
        "## get the value (pca0, book title)\n",
        "book_comp0 = [(f, books_dict[i]) for f,i in zip(pca0, list(books_dict.keys()))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2M1WZ7PDknt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we look at the two extremes on this axis, we will get the following results. **Ideally with a good model, we should find the top ten books sharing a certain feature like (genre, movies, etc.), while the lowest ten should be on the opposite side.**\n"
      ]
    },
    {
      "metadata": {
        "id": "6XJ-_Wqa_V9M",
        "colab_type": "code",
        "outputId": "abe0b15f-0988-4fba-9c55-7ebae5cfb34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "## books corresponding to the highest values of pca0\n",
        "sorted(book_comp0, key = itemgetter(0), reverse = True)[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.039083663, 'Der Schweizerische Robinson'),\n",
              " (0.03763531, 'Abhorsen'),\n",
              " (0.037420202, 'Traveling Mercies: Some Thoughts on Faith'),\n",
              " (0.036263224, 'Lola and the Boy Next Door'),\n",
              " (0.035904933, 'The Reluctant Fundamentalist'),\n",
              " (0.034426596, 'The Golden Fool'),\n",
              " (0.033787396, 'Anna and the French Kiss'),\n",
              " (0.032591347, 'Big Nate on a Roll'),\n",
              " (0.03244199, 'A Court of Thorns and Roses'),\n",
              " (0.03244152, 'Calico Joe')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "HyD5DclR_Xwj",
        "colab_type": "code",
        "outputId": "a7112b06-cc6a-46e4-8801-8dea5010ad31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "## books corresponding to the lowest values of pca0\n",
        "sorted(book_comp0, key = itemgetter(0))[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-0.03538448, nan),\n",
              " (-0.03486702, 'The Bride'),\n",
              " (-0.03396777, 'Trump: The Art of the Deal'),\n",
              " (-0.03219103, 'The Unexpected Mrs. Pollifax'),\n",
              " (-0.031592738, nan),\n",
              " (-0.031210287, nan),\n",
              " (-0.03081872, 'The Road to Little Dribbling'),\n",
              " (-0.030535161, 'Batman: The Black Mirror'),\n",
              " (-0.030096786,\n",
              "  'The Surgeon of Crowthorne: a tale of murder, madness & the love of words'),\n",
              " (-0.029848108,\n",
              "  'Amusing Ourselves to Death: Public Discourse in the Age of Show Business')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "MDehKNCDOCDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2- Tabular data method"
      ]
    },
    {
      "metadata": {
        "id": "8A7ZHSWWtCac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this method, we will just have users and books embedding layers that can have different dimensions. We will concatenate them together as if we have a table with `dim_embedding_user+dim_embedding_book` features. Then we can add dense layers, or even use these weights as features with other algorithms. "
      ]
    },
    {
      "metadata": {
        "id": "0yn0s9JrgnOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define model"
      ]
    },
    {
      "metadata": {
        "id": "8hPe3uFe33pM",
        "colab_type": "code",
        "outputId": "56782d03-601c-4454-8bed-44caf22839c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "## define the number of latent factors (can be different for the users and books)\n",
        "dim_embedding_user = 32\n",
        "dim_embedding_book = 32\n",
        "\n",
        "## book embedding\n",
        "book_input= Input(shape=[1], name='Book')\n",
        "book_embedding = Embedding(n_books + 1, dim_embedding_book, name='Book-Embedding')(book_input)\n",
        "book_vec = Flatten(name='Book-Flatten')(book_embedding)\n",
        "book_vec = Dropout(0.2)(book_vec)\n",
        "\n",
        "## user embedding\n",
        "user_input = Input(shape=[1], name='User')\n",
        "user_embedding = Embedding(n_users + 1, dim_embedding_user, name ='User-Embedding')(user_input)\n",
        "user_vec = Flatten(name ='User-Flatten')(user_embedding)\n",
        "user_vec = Dropout(0.2)(user_vec)\n",
        "\n",
        "## concatenate flattened values \n",
        "concat = concatenate([book_vec, user_vec])\n",
        "concat_dropout = Dropout(0.2)(concat)\n",
        "\n",
        "## add dense layer (can try more)\n",
        "dense_1 = Dense(20, name ='Fully-Connected1', activation='relu')(concat)\n",
        "\n",
        "## define output (can try sigmoid instead of relu)\n",
        "result = Dense(1, activation ='relu',name ='Activation')(dense_1)\n",
        "\n",
        "## define model with 2 inputs and 1 output\n",
        "model_tabular = Model([user_input, book_input], result)\n",
        "\n",
        "## show model summary\n",
        "model_tabular.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Book (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "User (InputLayer)               (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Book-Embedding (Embedding)      (None, 1, 32)        320032      Book[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "User-Embedding (Embedding)      (None, 1, 32)        1709600     User[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "Book-Flatten (Flatten)          (None, 32)           0           Book-Embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "User-Flatten (Flatten)          (None, 32)           0           User-Embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 32)           0           Book-Flatten[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 32)           0           User-Flatten[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 64)           0           dropout_23[0][0]                 \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Fully-Connected1 (Dense)        (None, 20)           1300        concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Activation (Dense)              (None, 1)            21          Fully-Connected1[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,030,953\n",
            "Trainable params: 2,030,953\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l3hKTYgTgwT5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Show model diagram"
      ]
    },
    {
      "metadata": {
        "id": "dKmFRHIId8c-",
        "colab_type": "code",
        "outputId": "a151a55f-833d-41c9-aa0f-414e4ba318da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "cell_type": "code",
      "source": [
        "SVG(model_to_dot(model_tabular).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"483pt\" viewBox=\"0.00 0.00 412.00 483.00\" width=\"412pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 408,-479 408,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139737308477312 -->\n<g class=\"node\" id=\"node1\">\n<title>139737308477312</title>\n<polygon fill=\"none\" points=\"38,-438.5 38,-474.5 158,-474.5 158,-438.5 38,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98\" y=\"-452.8\">Book: InputLayer</text>\n</g>\n<!-- 139737308477424 -->\n<g class=\"node\" id=\"node3\">\n<title>139737308477424</title>\n<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 196,-401.5 196,-365.5 0,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98\" y=\"-379.8\">Book-Embedding: Embedding</text>\n</g>\n<!-- 139737308477312&#45;&gt;139737308477424 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139737308477312-&gt;139737308477424</title>\n<path d=\"M98,-438.4551C98,-430.3828 98,-420.6764 98,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"101.5001,-411.5903 98,-401.5904 94.5001,-411.5904 101.5001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308478040 -->\n<g class=\"node\" id=\"node2\">\n<title>139737308478040</title>\n<polygon fill=\"none\" points=\"252,-438.5 252,-474.5 366,-474.5 366,-438.5 252,-438.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-452.8\">User: InputLayer</text>\n</g>\n<!-- 139737308477592 -->\n<g class=\"node\" id=\"node4\">\n<title>139737308477592</title>\n<polygon fill=\"none\" points=\"214,-365.5 214,-401.5 404,-401.5 404,-365.5 214,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-379.8\">User-Embedding: Embedding</text>\n</g>\n<!-- 139737308478040&#45;&gt;139737308477592 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139737308478040-&gt;139737308477592</title>\n<path d=\"M309,-438.4551C309,-430.3828 309,-420.6764 309,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"312.5001,-411.5903 309,-401.5904 305.5001,-411.5904 312.5001,-411.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139736936708192 -->\n<g class=\"node\" id=\"node5\">\n<title>139736936708192</title>\n<polygon fill=\"none\" points=\"54.5,-292.5 54.5,-328.5 193.5,-328.5 193.5,-292.5 54.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"124\" y=\"-306.8\">Book-Flatten: Flatten</text>\n</g>\n<!-- 139737308477424&#45;&gt;139736936708192 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139737308477424-&gt;139736936708192</title>\n<path d=\"M104.427,-365.4551C107.3645,-357.2074 110.9097,-347.2536 114.1712,-338.0962\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"117.4987,-339.185 117.5569,-328.5904 110.9045,-336.8364 117.4987,-339.185\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308477480 -->\n<g class=\"node\" id=\"node6\">\n<title>139737308477480</title>\n<polygon fill=\"none\" points=\"238.5,-292.5 238.5,-328.5 371.5,-328.5 371.5,-292.5 238.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305\" y=\"-306.8\">User-Flatten: Flatten</text>\n</g>\n<!-- 139737308477592&#45;&gt;139737308477480 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139737308477592-&gt;139737308477480</title>\n<path d=\"M308.0112,-365.4551C307.5689,-357.3828 307.0371,-347.6764 306.5442,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"310.0332,-338.3839 305.9913,-328.5904 303.0437,-338.7669 310.0332,-338.3839\" stroke=\"#000000\"/>\n</g>\n<!-- 139736936708360 -->\n<g class=\"node\" id=\"node7\">\n<title>139736936708360</title>\n<polygon fill=\"none\" points=\"66.5,-219.5 66.5,-255.5 207.5,-255.5 207.5,-219.5 66.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"137\" y=\"-233.8\">dropout_23: Dropout</text>\n</g>\n<!-- 139736936708192&#45;&gt;139736936708360 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139736936708192-&gt;139736936708360</title>\n<path d=\"M127.2135,-292.4551C128.651,-284.3828 130.3795,-274.6764 131.9813,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"135.4709,-266.0491 133.7784,-255.5904 128.5793,-264.8218 135.4709,-266.0491\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308477648 -->\n<g class=\"node\" id=\"node8\">\n<title>139737308477648</title>\n<polygon fill=\"none\" points=\"231.5,-219.5 231.5,-255.5 372.5,-255.5 372.5,-219.5 231.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"302\" y=\"-233.8\">dropout_24: Dropout</text>\n</g>\n<!-- 139737308477480&#45;&gt;139737308477648 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139737308477480-&gt;139737308477648</title>\n<path d=\"M304.2584,-292.4551C303.9267,-284.3828 303.5278,-274.6764 303.1582,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"306.6512,-265.4382 302.7434,-255.5904 299.6571,-265.7257 306.6512,-265.4382\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308449872 -->\n<g class=\"node\" id=\"node9\">\n<title>139737308449872</title>\n<polygon fill=\"none\" points=\"128.5,-146.5 128.5,-182.5 303.5,-182.5 303.5,-146.5 128.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216\" y=\"-160.8\">concatenate_8: Concatenate</text>\n</g>\n<!-- 139736936708360&#45;&gt;139737308449872 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139736936708360-&gt;139737308449872</title>\n<path d=\"M156.5281,-219.4551C166.3083,-210.4177 178.3052,-199.3319 188.9561,-189.4899\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"191.4536,-191.9476 196.4228,-182.5904 186.7029,-186.8065 191.4536,-191.9476\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308477648&#45;&gt;139737308449872 -->\n<g class=\"edge\" id=\"edge8\">\n<title>139737308477648-&gt;139737308449872</title>\n<path d=\"M280.7416,-219.4551C269.9914,-210.3299 256.781,-199.1165 245.1029,-189.2036\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"247.2007,-186.3934 237.3119,-182.5904 242.6707,-191.7301 247.2007,-186.3934\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308449480 -->\n<g class=\"node\" id=\"node10\">\n<title>139737308449480</title>\n<polygon fill=\"none\" points=\"134,-73.5 134,-109.5 298,-109.5 298,-73.5 134,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216\" y=\"-87.8\">Fully-Connected1: Dense</text>\n</g>\n<!-- 139737308449872&#45;&gt;139737308449480 -->\n<g class=\"edge\" id=\"edge9\">\n<title>139737308449872-&gt;139737308449480</title>\n<path d=\"M216,-146.4551C216,-138.3828 216,-128.6764 216,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.5001,-119.5903 216,-109.5904 212.5001,-119.5904 219.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 139737308449984 -->\n<g class=\"node\" id=\"node11\">\n<title>139737308449984</title>\n<polygon fill=\"none\" points=\"156.5,-.5 156.5,-36.5 275.5,-36.5 275.5,-.5 156.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"216\" y=\"-14.8\">Activation: Dense</text>\n</g>\n<!-- 139737308449480&#45;&gt;139737308449984 -->\n<g class=\"edge\" id=\"edge10\">\n<title>139737308449480-&gt;139737308449984</title>\n<path d=\"M216,-73.4551C216,-65.3828 216,-55.6764 216,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"219.5001,-46.5903 216,-36.5904 212.5001,-46.5904 219.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "s3Dq7s1wgsZe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Train model"
      ]
    },
    {
      "metadata": {
        "id": "ScYV3ahVZ0CL",
        "colab_type": "code",
        "outputId": "2ccdaa3e-e56d-4928-ad43-36e0e5afad9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "## specify learning rate (or use the default by specifying optimizer = 'adam')\n",
        "opt_adam = Adam(lr = 0.002)\n",
        "\n",
        "## compile model\n",
        "model_tabular.compile(optimizer= opt_adam, loss= ['mse'], metrics=['mean_absolute_error'])\n",
        "\n",
        "## fit model\n",
        "history_tabular = model_tabular.fit([train['user_id'], train['book_id']],\n",
        "                                    train['rating'],\n",
        "                                    batch_size = 256,\n",
        "                                    validation_split = 0.005,\n",
        "                                    epochs = 16,\n",
        "                                    verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5351936 samples, validate on 26895 samples\n",
            "Epoch 1/16\n",
            "5351936/5351936 [==============================] - 165s 31us/step - loss: 0.8301 - mean_absolute_error: 0.7055 - val_loss: 0.7267 - val_mean_absolute_error: 0.6621\n",
            "Epoch 2/16\n",
            "5351936/5351936 [==============================] - 165s 31us/step - loss: 0.7243 - mean_absolute_error: 0.6641 - val_loss: 0.7067 - val_mean_absolute_error: 0.6535\n",
            "Epoch 3/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6999 - mean_absolute_error: 0.6505 - val_loss: 0.6921 - val_mean_absolute_error: 0.6520\n",
            "Epoch 4/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6815 - mean_absolute_error: 0.6402 - val_loss: 0.6804 - val_mean_absolute_error: 0.6347\n",
            "Epoch 5/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6691 - mean_absolute_error: 0.6330 - val_loss: 0.6777 - val_mean_absolute_error: 0.6455\n",
            "Epoch 6/16\n",
            "5351936/5351936 [==============================] - 167s 31us/step - loss: 0.6595 - mean_absolute_error: 0.6276 - val_loss: 0.6709 - val_mean_absolute_error: 0.6394\n",
            "Epoch 7/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6520 - mean_absolute_error: 0.6231 - val_loss: 0.6674 - val_mean_absolute_error: 0.6298\n",
            "Epoch 8/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6462 - mean_absolute_error: 0.6197 - val_loss: 0.6670 - val_mean_absolute_error: 0.6296\n",
            "Epoch 9/16\n",
            "5351936/5351936 [==============================] - 165s 31us/step - loss: 0.6410 - mean_absolute_error: 0.6168 - val_loss: 0.6636 - val_mean_absolute_error: 0.6286\n",
            "Epoch 10/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6369 - mean_absolute_error: 0.6145 - val_loss: 0.6642 - val_mean_absolute_error: 0.6239\n",
            "Epoch 11/16\n",
            "5351936/5351936 [==============================] - 164s 31us/step - loss: 0.6329 - mean_absolute_error: 0.6122 - val_loss: 0.6669 - val_mean_absolute_error: 0.6392\n",
            "Epoch 12/16\n",
            "5351936/5351936 [==============================] - 165s 31us/step - loss: 0.6298 - mean_absolute_error: 0.6104 - val_loss: 0.6621 - val_mean_absolute_error: 0.6284\n",
            "Epoch 13/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6268 - mean_absolute_error: 0.6087 - val_loss: 0.6678 - val_mean_absolute_error: 0.6392\n",
            "Epoch 14/16\n",
            "5351936/5351936 [==============================] - 166s 31us/step - loss: 0.6243 - mean_absolute_error: 0.6073 - val_loss: 0.6593 - val_mean_absolute_error: 0.6289\n",
            "Epoch 15/16\n",
            "5351936/5351936 [==============================] - 165s 31us/step - loss: 0.6221 - mean_absolute_error: 0.6060 - val_loss: 0.6616 - val_mean_absolute_error: 0.6228\n",
            "Epoch 16/16\n",
            "5351936/5351936 [==============================] - 164s 31us/step - loss: 0.6196 - mean_absolute_error: 0.6047 - val_loss: 0.6608 - val_mean_absolute_error: 0.6215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ubyuwMcempBf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model performs better that the first one after 4 epochs. Part of the experimentation is to train both for more epochs,  tune the hyperparamers or modify the architecture."
      ]
    },
    {
      "metadata": {
        "id": "Y0KrfuXVwCQQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predict ratings of test data"
      ]
    },
    {
      "metadata": {
        "id": "_egnxeBtwKhz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we want to check the model, we can use the test data which gives `mse` close to the validation loss."
      ]
    },
    {
      "metadata": {
        "id": "bH1mY6EBTFPX",
        "colab_type": "code",
        "outputId": "736d9369-e6f8-45db-b745-d66034f9523e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## import libraries\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "## define a function to return arrays in the required form \n",
        "def get_array(series):\n",
        "    return np.array([[element] for element in series])\n",
        "\n",
        "## predict on test data  \n",
        "predictions = model_tabular.predict([get_array(test['user_id']), get_array(test['book_id'])])\n",
        "\n",
        "f'mean squared error on test data is {mean_squared_error(test[\"rating\"], predictions)}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mean squared error on test data is 0.6727620986954058'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "OSmo-o-pgiWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3- Other methods"
      ]
    },
    {
      "metadata": {
        "id": "xZoZuniDgu57",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Although we saw preliminary models we could get the idea behind each of the methods and we have a starting point to experiment more. There's also another approach which almost combines both methods. It was published in a paper entitled [Neural Collaborative Filtering](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf). It is not included in this notebook but it was worth mentioning as we discuss similar methods."
      ]
    },
    {
      "metadata": {
        "id": "OtWHL5PCfyGw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ![](https://github.com/OmaymaS/onceupondata/blob/master/static/post/2019-02-01_goodreads-book-recommendation/nn_paper.png?raw=true)\n",
        "*Source: [Neural Collaborative Filtering](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf), Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.*"
      ]
    },
    {
      "metadata": {
        "id": "7OLf5BYOjMHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References and Readings\n",
        "\n",
        "- Practical Deep Learning for Coders [Lesson 4: NLP, Tabular data, Collaborative filtering, Embeddings](https://course.fast.ai/videos/?lesson=4), Jeremy Howard, fast.ai\n",
        "-  [Understanding Latent Style](https://multithreaded.stitchfix.com/blog/2018/06/28/latent-style/), Erin Boyle and Jana Beck, StichFix\n",
        "- [Neural Collaborative Filtering](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf), Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.\n"
      ]
    }
  ]
}